{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "982f0ad9",
   "metadata": {},
   "source": [
    "This is pulled from https://github.com/snap-stanford/ogb/tree/master/examples/lsc/pcqm4m\n",
    "\n",
    "dependencies:\n",
    "- install rdkit (I used conda) https://www.rdkit.org/docs/Install.html\n",
    "1. conda create -c conda-forge -n my-rdkit-env rdkit\n",
    "2. conda activate my-rdkit-env\n",
    "3. cd [anaconda folder]/bin\n",
    "4. source activate my-rdkit-env\n",
    "\n",
    "- install ogb, torch and pytorch-geometric\n",
    "\n",
    "- run the main-gnn.py code to download the dataset, extract and train (see the readme.md). I could finish extracting because my RAM wasn't enough. If you face the some problem, use this notebook to extract just a handful of the dataset. This code is taken from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ecd473b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "from ogb.lsc import DglPCQM4MDataset, PCQM4MEvaluator\n",
    "\n",
    "import argparse\n",
    "import dgl\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from gnn import GNN\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import shutil\n",
    "from ogb.utils import smiles2graph\n",
    "from ogb.utils.torch_util import replace_numpy_with_torchtensor\n",
    "from ogb.utils.url import decide_download, download_url, extract_zip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dgl.data.utils import load_graphs, save_graphs, Subset\n",
    "import dgl\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "reg_criterion = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79849beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir log\n",
    "# !mkdir checkpoint\n",
    "# !mkdir test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "683d1e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python main_gnn.py --gnn gcn --log_dir log --checkpoint_dir checkpoint --save_test_dir test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b7d107a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>smiles</th>\n",
       "      <th>homolumogap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Cc1ccc(cc1)C1C=c2cnccc2=NC1=O</td>\n",
       "      <td>3.047675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>COc1cc(OC)ccc1C=CN(C(=O)C)C</td>\n",
       "      <td>4.410966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>C=CCN(C(=O)C)C=Cc1ccccc1C</td>\n",
       "      <td>4.639541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>C=CCN(C(=O)C)C=Cc1ccccc1F</td>\n",
       "      <td>4.492600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>C=CCN(C(=O)C)C=Cc1ccccc1Cl</td>\n",
       "      <td>4.612330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3803448</th>\n",
       "      <td>3803448</td>\n",
       "      <td>O=N(=O)c1ccc(c(c1)N(=O)=O)Cl</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3803449</th>\n",
       "      <td>3803449</td>\n",
       "      <td>NCC(=O)COP(=O)(O)O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3803450</th>\n",
       "      <td>3803450</td>\n",
       "      <td>CC(CN)O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3803451</th>\n",
       "      <td>3803451</td>\n",
       "      <td>OC1C=CC=C(C1O)C(=O)O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3803452</th>\n",
       "      <td>3803452</td>\n",
       "      <td>[O-]C(=O)CC(C[N+](C)(C)C)OC(=O)C</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3803453 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             idx                            smiles  homolumogap\n",
       "0              0     Cc1ccc(cc1)C1C=c2cnccc2=NC1=O     3.047675\n",
       "1              1       COc1cc(OC)ccc1C=CN(C(=O)C)C     4.410966\n",
       "2              2         C=CCN(C(=O)C)C=Cc1ccccc1C     4.639541\n",
       "3              3         C=CCN(C(=O)C)C=Cc1ccccc1F     4.492600\n",
       "4              4        C=CCN(C(=O)C)C=Cc1ccccc1Cl     4.612330\n",
       "...          ...                               ...          ...\n",
       "3803448  3803448      O=N(=O)c1ccc(c(c1)N(=O)=O)Cl          NaN\n",
       "3803449  3803449                NCC(=O)COP(=O)(O)O          NaN\n",
       "3803450  3803450                           CC(CN)O          NaN\n",
       "3803451  3803451              OC1C=CC=C(C1O)C(=O)O          NaN\n",
       "3803452  3803452  [O-]C(=O)CC(C[N+](C)(C)C)OC(=O)C          NaN\n",
       "\n",
       "[3803453 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "my folder structure looks like (obtained from running main_gnn)\n",
    "dataset/\n",
    "    pcqm4m_kddcup2021/\n",
    "        mapping/\n",
    "        processed/\n",
    "        raw/\n",
    "            data.csv.gz\n",
    "\"\"\"\n",
    "\n",
    "ROOT = \"dataset/pcqm4m_kddcup2021\"\n",
    "filename = \"{}/{}\".format(ROOT, \"raw/data.csv.gz\")\n",
    "data_df = pd.read_csv(filename)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d237fb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load Train/Test/Valid split dictionary\n",
    "\"\"\"\n",
    "# INPUT PARAMETERS HERE: SET TRAIN, TEST AND VAL SIZE\n",
    "trainSize = 50000\n",
    "testSize = 10000\n",
    "valSize = 10000\n",
    "\n",
    "# load raw split dict\n",
    "split_dict = torch.load(osp.join(ROOT, 'split_dict.pt'))\n",
    "# load new split dict\n",
    "new_split_dict = split_dict.copy()\n",
    "\n",
    "# sample new split dict\n",
    "new_split_dict[\"train\"] = np.random.choice(new_split_dict[\"train\"], size=trainSize, replace=False)\n",
    "new_split_dict[\"test\"] = np.random.choice(new_split_dict[\"test\"], size=testSize, replace=False)\n",
    "new_split_dict[\"valid\"] = np.random.choice(new_split_dict[\"valid\"], size=valSize, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de1c1f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract train, test and valid indices\n",
    "sampleIndices = np.append(\n",
    "    new_split_dict[\"train\"], [new_split_dict[\"test\"], new_split_dict[\"valid\"]]\n",
    ")\n",
    "\n",
    "# remap indices\n",
    "idMapping = {}\n",
    "for i, index in enumerate(sampleIndices):\n",
    "    idMapping[index] = i\n",
    "    \n",
    "# idMapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8582d4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eek31\\AppData\\Local\\Continuum\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\eek31\\AppData\\Local\\Continuum\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>smiles</th>\n",
       "      <th>homolumogap</th>\n",
       "      <th>old_idx</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>CC(Cc1onc(n1)CNCC(=O)O)C</td>\n",
       "      <td>5.959293</td>\n",
       "      <td>2365809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CCOC(=O)C=CC(CO)(C)C</td>\n",
       "      <td>6.136167</td>\n",
       "      <td>2108181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>CN(c1scnn1)Cc1ccc(cc1)N</td>\n",
       "      <td>5.268124</td>\n",
       "      <td>979050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>CCCNC(CCc1c[nH]cc1)C</td>\n",
       "      <td>6.805567</td>\n",
       "      <td>547477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>COCCCn1nnc(c1CC)C(=O)O</td>\n",
       "      <td>6.476310</td>\n",
       "      <td>1390460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>69995</td>\n",
       "      <td>COC(=O)C1CN2C(C1)CCC2</td>\n",
       "      <td>5.910313</td>\n",
       "      <td>3211746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>69996</td>\n",
       "      <td>CCCC(NC(=O)c1ccc(cc1)OC)C#N</td>\n",
       "      <td>5.371527</td>\n",
       "      <td>3107244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>69997</td>\n",
       "      <td>OC(=O)C[C@H](C1=C(C)C(=O)OC1=O)O</td>\n",
       "      <td>4.157900</td>\n",
       "      <td>3385545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>69998</td>\n",
       "      <td>CC(=O)C=CNCCNC(=CC(=O)C)C</td>\n",
       "      <td>4.892607</td>\n",
       "      <td>3382577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>69999</td>\n",
       "      <td>Cc1cnc(c(n1)F)C</td>\n",
       "      <td>5.534796</td>\n",
       "      <td>3402850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         idx                            smiles  homolumogap  old_idx\n",
       "idx                                                                 \n",
       "0          0          CC(Cc1onc(n1)CNCC(=O)O)C     5.959293  2365809\n",
       "1          1              CCOC(=O)C=CC(CO)(C)C     6.136167  2108181\n",
       "2          2           CN(c1scnn1)Cc1ccc(cc1)N     5.268124   979050\n",
       "3          3              CCCNC(CCc1c[nH]cc1)C     6.805567   547477\n",
       "4          4            COCCCn1nnc(c1CC)C(=O)O     6.476310  1390460\n",
       "...      ...                               ...          ...      ...\n",
       "69995  69995             COC(=O)C1CN2C(C1)CCC2     5.910313  3211746\n",
       "69996  69996       CCCC(NC(=O)c1ccc(cc1)OC)C#N     5.371527  3107244\n",
       "69997  69997  OC(=O)C[C@H](C1=C(C)C(=O)OC1=O)O     4.157900  3385545\n",
       "69998  69998         CC(=O)C=CNCCNC(=CC(=O)C)C     4.892607  3382577\n",
       "69999  69999                   Cc1cnc(c(n1)F)C     5.534796  3402850\n",
       "\n",
       "[70000 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Save New Dataframe based on smaller samples\n",
    "\"\"\"\n",
    "# filter dataframe\n",
    "sampleDF = data_df.iloc[sampleIndices]\n",
    "sampleDF[\"old_idx\"] = sampleDF[\"idx\"]\n",
    "# remap the index\n",
    "sampleDF[\"idx\"] = sampleDF[\"idx\"].apply(lambda x: idMapping[x])\n",
    "sampleDF.index = sampleDF[\"idx\"]\n",
    "\n",
    "# save sample dataframe\n",
    "sampleDF.to_csv(osp.join(ROOT, \"raw\", \"sample_data.csv.gz\"))\n",
    "\n",
    "sampleDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8903af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Remap split_dict indices\n",
    "\"\"\"\n",
    "new_split_dict[\"train\"] = np.array([idMapping[x] for x in new_split_dict[\"train\"]])\n",
    "new_split_dict[\"test\"] = np.array([idMapping[x] for x in new_split_dict[\"test\"]])\n",
    "new_split_dict[\"valid\"] = np.array([idMapping[x] for x in new_split_dict[\"valid\"]])\n",
    "\n",
    "torch.save(new_split_dict, osp.join(ROOT, \"sample_split_dict.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e8c892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "370e16e3",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbfba40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_dgl(samples):\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    return batched_graph, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ca694fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ogb.lsc import DglPCQM4MDataset, PCQM4MEvaluator\n",
    "\n",
    "class SampleDglPCQM4MDataset(DglPCQM4MDataset):\n",
    "    \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return 'sample_data.csv.gz'\n",
    "\n",
    "    def prepare_graph(self):\n",
    "        processed_dir = osp.join(self.folder, 'processed')\n",
    "        raw_dir = osp.join(self.folder, 'raw')\n",
    "        pre_processed_file_path = osp.join(processed_dir, 'dgl_data_processed')\n",
    "\n",
    "        if osp.exists(pre_processed_file_path):        \n",
    "            # if pre-processed file already exists\n",
    "            self.graphs, label_dict = load_graphs(pre_processed_file_path)\n",
    "            self.labels = label_dict['labels']\n",
    "        else:\n",
    "            # if pre-processed file does not exist\n",
    "            \n",
    "            if not osp.exists(osp.join(raw_dir, 'sample_data.csv.gz')):\n",
    "                # if the raw file does not exist, then download it.\n",
    "                self.download()\n",
    "\n",
    "            data_df = pd.read_csv(osp.join(raw_dir, 'sample_data.csv.gz'))\n",
    "            smiles_list = data_df['smiles']\n",
    "            homolumogap_list = data_df['homolumogap']\n",
    "\n",
    "            print('Converting SMILES strings into graphs...')\n",
    "            self.graphs = []\n",
    "            self.labels = []\n",
    "            for i in tqdm(range(len(smiles_list))):\n",
    "\n",
    "                smiles = smiles_list[i]\n",
    "                homolumogap = homolumogap_list[i]\n",
    "                graph = self.smiles2graph(smiles)\n",
    "                \n",
    "                assert(len(graph['edge_feat']) == graph['edge_index'].shape[1])\n",
    "                assert(len(graph['node_feat']) == graph['num_nodes'])\n",
    "\n",
    "                dgl_graph = dgl.graph((graph['edge_index'][0], graph['edge_index'][1]), num_nodes = graph['num_nodes'])\n",
    "                dgl_graph.edata['feat'] = torch.from_numpy(graph['edge_feat']).to(torch.int64)\n",
    "                dgl_graph.ndata['feat'] = torch.from_numpy(graph['node_feat']).to(torch.int64)\n",
    "\n",
    "                self.graphs.append(dgl_graph)\n",
    "                self.labels.append(homolumogap)\n",
    "\n",
    "            self.labels = torch.tensor(self.labels, dtype=torch.float32)\n",
    "\n",
    "            # double-check prediction target\n",
    "            split_dict = self.get_idx_split()\n",
    "            assert(all([not torch.isnan(self.labels[i]) for i in split_dict['train']]))\n",
    "            assert(all([not torch.isnan(self.labels[i]) for i in split_dict['valid']]))\n",
    "            assert(all([torch.isnan(self.labels[i]) for i in split_dict['test']]))\n",
    "\n",
    "            print('Saving...')\n",
    "            save_graphs(pre_processed_file_path, self.graphs, labels={'labels': self.labels})\n",
    "        \n",
    "    \n",
    "    # just modify the get_idx_split function to call our new filename\n",
    "    def get_idx_split(self):\n",
    "        # NOTE: CHANGED split_dict.pt to sample_split_dict.pt\n",
    "        split_dict = replace_numpy_with_torchtensor(torch.load(osp.join(self.folder, 'sample_split_dict.pt')))\n",
    "        return split_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83740713",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "isSample = True #False #\n",
    "\n",
    "if isSample:\n",
    "    dataset = SampleDglPCQM4MDataset(root = 'dataset/')\n",
    "else:\n",
    "#     !rm dataset/pcqm4m_kddcup2021/processed/geometric_data_processed.pt\n",
    "    dataset = DglPCQM4MDataset(root = 'dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ed0ce1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    0,     1,     2,  ..., 49997, 49998, 49999]),\n",
       " tensor([50000, 50001, 50002,  ..., 59997, 59998, 59999]),\n",
       " tensor([60000, 60001, 60002,  ..., 69997, 69998, 69999]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_idx = dataset.get_idx_split()\n",
    "split_idx[\"train\"] = split_idx[\"train\"].type(torch.LongTensor)\n",
    "split_idx[\"test\"] = split_idx[\"test\"].type(torch.LongTensor)\n",
    "split_idx[\"valid\"] = split_idx[\"valid\"].type(torch.LongTensor)\n",
    "\n",
    "split_idx[\"train\"], split_idx[\"test\"], split_idx[\"valid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b668ac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = PCQM4MEvaluator()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7dbea499",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "GET ARGUMENTS FROM EXAMPLE\n",
    "\"\"\"\n",
    "\n",
    "python main_gnn.py --gnn gcn --log_dir $LOG_DIR --checkpoint_dir $CHECKPOINT_DIR --save_test_dir $TEST_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49aefc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get args from main_gnn CLI\n",
    "class Argument(object):\n",
    "    name = \"args\"\n",
    "    \n",
    "args = Argument()\n",
    "args.batch_size = 256\n",
    "args.num_workers = 0\n",
    "args.num_layers = 5\n",
    "args.emb_dim = 600\n",
    "args.drop_ratio = 0\n",
    "args.graph_pooling = \"sum\"\n",
    "args.checkpoint_dir = \"checkpoint\"\n",
    "args.device = 0\n",
    "args.log_dir = \"models/gin-virtual/log\"\n",
    "args.checkpoint_dir = \"models/gin-virtual/checkpoint\"\n",
    "args.train_subset = False\n",
    "args.epochs = 1\n",
    "args.save_test_dir = \"models/gin-virtual/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c96ed39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4d2de57",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset[split_idx[\"train\"]], batch_size=args.batch_size, shuffle=True, num_workers = args.num_workers, collate_fn=collate_dgl)\n",
    "valid_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=args.batch_size, shuffle=False, num_workers = args.num_workers, collate_fn=collate_dgl)\n",
    "test_loader = DataLoader(dataset[split_idx[\"test\"]], batch_size=args.batch_size, shuffle=False, num_workers = args.num_workers, collate_fn=collate_dgl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff3af0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_params = {\n",
    "    'num_layers': args.num_layers,\n",
    "    'emb_dim': args.emb_dim,\n",
    "    'drop_ratio': args.drop_ratio,\n",
    "    'graph_pooling': args.graph_pooling\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36108172",
   "metadata": {},
   "source": [
    "## default model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "105bfba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnn import GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d09b31c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNN(gnn_type='gin', virtual_node=True, **shared_params).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2757f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing weights from models/gin-virtual/checkpoint\\checkpoint.pt\n"
     ]
    }
   ],
   "source": [
    "# check if checkpoint exist -> load model\n",
    "checkpointFile = os.path.join(args.checkpoint_dir, 'checkpoint.pt')\n",
    "if os.path.exists(checkpointFile):\n",
    "    # load weights\n",
    "    print(\"Loading existing weights from {}\".format(checkpointFile))\n",
    "    checkpointData = torch.load(checkpointFile)\n",
    "    model.load_state_dict(checkpointData[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b293c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import train, eval, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2af52bc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|                                                                               | 0/196 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Params: 6656406\n",
      "=====Epoch 1\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:51<00:00,  3.82it/s]\n",
      "Iteration:   2%|â–ˆâ–Š                                                                      | 1/40 [00:00<00:04,  9.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:05<00:00,  7.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train': 0.40218543139647467, 'Validation': 0.35926583409309387}\n",
      "Saving checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   2%|â–ˆâ–Š                                                                      | 1/40 [00:00<00:05,  7.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:05<00:00,  7.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation MAE so far: 0.35926583409309387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'#Params: {num_params}')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "if args.log_dir is not '':\n",
    "    writer = SummaryWriter(log_dir=args.log_dir)\n",
    "\n",
    "best_valid_mae = 1000\n",
    "\n",
    "if args.train_subset:\n",
    "    scheduler = StepLR(optimizer, step_size=300, gamma=0.25)\n",
    "    args.epochs = 1000\n",
    "else:\n",
    "    scheduler = StepLR(optimizer, step_size=30, gamma=0.25)\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    print(\"=====Epoch {}\".format(epoch))\n",
    "    print('Training...')\n",
    "    train_mae = train(model, device, train_loader, optimizer)\n",
    "\n",
    "    print('Evaluating...')\n",
    "    valid_mae = eval(model, device, valid_loader, evaluator)\n",
    "\n",
    "    print({'Train': train_mae, 'Validation': valid_mae})\n",
    "\n",
    "    if args.log_dir is not '':\n",
    "        writer.add_scalar('valid/mae', valid_mae, epoch)\n",
    "        writer.add_scalar('train/mae', train_mae, epoch)\n",
    "\n",
    "    if valid_mae < best_valid_mae:\n",
    "        best_valid_mae = valid_mae\n",
    "        if args.checkpoint_dir is not '':\n",
    "            print('Saving checkpoint...')\n",
    "            checkpoint = {'epoch': epoch, 'model_state_dict': model.state_dict(),\n",
    "                          'optimizer_state_dict': optimizer.state_dict(),\n",
    "                          'scheduler_state_dict': scheduler.state_dict(), 'best_val_mae': best_valid_mae,\n",
    "                          'num_params': num_params}\n",
    "            torch.save(checkpoint, os.path.join(args.checkpoint_dir, 'checkpoint.pt'))\n",
    "\n",
    "        if args.save_test_dir is not '':\n",
    "            print('Predicting on test data...')\n",
    "            y_pred = test(model, device, test_loader)\n",
    "#             print('Saving test submission file...')\n",
    "#             evaluator.save_test_submission({'y_pred': y_pred}, args.save_test_dir)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f'Best validation MAE so far: {best_valid_mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceae5d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26cece67",
   "metadata": {},
   "source": [
    "## DiffPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16d14e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import block_diag\n",
    "\n",
    "import dgl\n",
    "\n",
    "from model.dgl_layers import GraphSage, GraphSageLayer, DiffPoolBatchedGraphLayer\n",
    "from model.tensorized_layers import *\n",
    "from model.model_utils import batch2tensor\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9b8caae-538e-4fb8-bc7c-536dbbf93891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.linalg import block_diag\n",
    "\n",
    "import dgl.function as fn\n",
    "\n",
    "from model.dgl_layers.aggregator import MaxPoolAggregator, MeanAggregator, LSTMAggregator\n",
    "from model.dgl_layers.bundler import Bundler\n",
    "from model.model_utils import masked_softmax\n",
    "from model.loss import EntropyLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82af459d-8789-4f72-98f5-bcf2bbc87577",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSageLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    GraphSage layer in Inductive learning paper by hamilton\n",
    "    Here, graphsage layer is a reduced function in DGL framework\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_feats, out_feats, activation, dropout,\n",
    "                 aggregator_type, bn=False, bias=True):\n",
    "        super(GraphSageLayer, self).__init__()\n",
    "        self.use_bn = bn\n",
    "        self.bundler = Bundler(in_feats, out_feats, activation, dropout,\n",
    "                               bias=bias)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        if aggregator_type == \"maxpool\":\n",
    "            self.aggregator = MaxPoolAggregator(in_feats, in_feats,\n",
    "                                                activation, bias)\n",
    "        elif aggregator_type == \"lstm\":\n",
    "            self.aggregator = LSTMAggregator(in_feats, in_feats)\n",
    "        else:\n",
    "            self.aggregator = MeanAggregator()\n",
    "\n",
    "    # edge_attr not used\n",
    "    def forward(self, g, h):\n",
    "        h = self.dropout(h)\n",
    "        g.ndata['h'] = h\n",
    "        if self.use_bn and not hasattr(self, 'bn'):\n",
    "            device = h.device\n",
    "            self.bn = nn.BatchNorm1d(h.size()[1]).to(device)\n",
    "        g.update_all(fn.copy_src(src='h', out='m'), self.aggregator,\n",
    "                     self.bundler)\n",
    "        if self.use_bn:\n",
    "            h = self.bn(h)\n",
    "        h = g.ndata.pop('h')\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd4ac9eb-648b-4b23-8553-48db67fc2896",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffPoolBatchedGraphLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, assign_dim, output_feat_dim,\n",
    "                 activation, dropout, aggregator_type, link_pred):\n",
    "        super(DiffPoolBatchedGraphLayer, self).__init__()\n",
    "        self.embedding_dim = input_dim\n",
    "        self.assign_dim = assign_dim\n",
    "        self.hidden_dim = output_feat_dim\n",
    "        self.link_pred = link_pred\n",
    "        \n",
    "        self.feat_gc = GraphSageLayer(\n",
    "            input_dim,\n",
    "            output_feat_dim,\n",
    "            activation,\n",
    "            dropout,\n",
    "            aggregator_type)\n",
    "\n",
    "        self.pool_gc = GraphSageLayer(\n",
    "            input_dim,\n",
    "            assign_dim,\n",
    "            activation,\n",
    "            dropout,\n",
    "            aggregator_type)\n",
    "        \n",
    "        self.reg_loss = nn.ModuleList([])\n",
    "        self.loss_log = {}\n",
    "        self.reg_loss.append(EntropyLoss())\n",
    "\n",
    "    def forward(self, g, h):\n",
    "#         print(\"DiffPoolBatchedGraphLayer forward\")\n",
    "        feat = self.feat_gc(g, h)  # size = (sum_N, F_out), sum_N is num of nodes in this batch\n",
    "#         print(feat.shape)\n",
    "        device = feat.device\n",
    "        assign_tensor = self.pool_gc(g, h)  # size = (sum_N, N_a), N_a is num of nodes in pooled graph.\n",
    "        assign_tensor = F.softmax(assign_tensor, dim=1)\n",
    "        assign_tensor = torch.split(assign_tensor, g.batch_num_nodes().tolist())\n",
    "        assign_tensor = torch.block_diag(*assign_tensor)  # size = (sum_N, batch_size * N_a)\n",
    "\n",
    "        h = torch.matmul(torch.t(assign_tensor), feat)\n",
    "        adj = g.adjacency_matrix(transpose=False, ctx=device)\n",
    "        adj_new = torch.sparse.mm(adj, assign_tensor)\n",
    "        adj_new = torch.mm(torch.t(assign_tensor), adj_new)\n",
    "\n",
    "        if self.link_pred:\n",
    "            current_lp_loss = torch.norm(adj.to_dense() -\n",
    "                                         torch.mm(assign_tensor, torch.t(assign_tensor))) / np.power(g.number_of_nodes(), 2)\n",
    "            self.loss_log['LinkPredLoss'] = current_lp_loss\n",
    "\n",
    "        for loss_layer in self.reg_loss:\n",
    "            loss_name = str(type(loss_layer).__name__)\n",
    "            self.loss_log[loss_name] = loss_layer(adj, adj_new, assign_tensor)\n",
    "\n",
    "        return adj_new, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d13fcb68-6bfa-43ff-9231-43a52e076fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_diffpool_layer = DiffPoolBatchedGraphLayer(\n",
    "    input_dim=600, # graph embedding dimension\n",
    "    assign_dim=5, # group to 10\n",
    "    output_feat_dim=600,\n",
    "    activation=F.relu,\n",
    "    dropout=0.0,\n",
    "    aggregator_type=\"meanpool\",\n",
    "    link_pred=False\n",
    ").to(device)\n",
    "\n",
    "gc_after_pool = BatchedGraphSAGE(600, 600).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb509b2f-7bb6-4ed0-affc-c353a05d3b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from dgl.nn.pytorch import SumPooling, AvgPooling, MaxPooling, GlobalAttentionPooling, Set2Set\n",
    "\n",
    "from conv import GNN_node, GNN_node_Virtualnode\n",
    "\n",
    "from gnn import GNN\n",
    "\n",
    "class DiffPoolGNN(GNN):\n",
    "    def __init__(self, num_tasks = 1, num_layers = 5, emb_dim = 300, gnn_type = 'gin',\n",
    "                 virtual_node = True, residual = False, drop_ratio = 0, JK = \"last\",\n",
    "                 graph_pooling = \"sum\"):\n",
    "        super(DiffPoolGNN, self).__init__(num_tasks, num_layers, emb_dim, gnn_type,\n",
    "                 virtual_node, residual, drop_ratio, JK, graph_pooling)\n",
    "        \n",
    "        # 2x number of outputs\n",
    "        self.graph_pred_linear = nn.Linear(2*self.emb_dim, self.num_tasks)\n",
    "#         self.graph_pred_linear = nn.Linear(self.emb_dim, self.num_tasks)\n",
    "\n",
    "    def forward(self, g, x, edge_attr):\n",
    "        # 1. GCN: 3628x9 -> 3628x600\n",
    "        g.ndata['h'] = x\n",
    "        h_node = self.gnn_node(g, x, edge_attr)\n",
    "#         print(h_node.shape)\n",
    "\n",
    "        # 2. Graph Pooling 256x600\n",
    "        h_graph_1 = self.pool(g, h_node)\n",
    "#         print(h_graph.shape)\n",
    "        \n",
    "        # 3. DiffPool: (1280x1280), (1280x600)\n",
    "        adj, h_node = first_diffpool_layer(g, h_node)\n",
    "#         print(adj.shape, h_node.shape)\n",
    "        \n",
    "        # 3b. split to batches\n",
    "        node_per_pool_graph = int(adj.size()[0] / len(g.batch_num_nodes()))\n",
    "        h_node, adj = batch2tensor(adj, h_node, node_per_pool_graph)\n",
    "#         print(adj.shape, h_node.shape)\n",
    "\n",
    "        # 4. GCN:\n",
    "        h_node = self.gcn_forward_tensorized(h_node, adj, [gc_after_pool], True)\n",
    "#         print(h_node.shape)\n",
    "\n",
    "        # 5. Graph Pooling 256x600\n",
    "        h_graph_2 = torch.sum(h_node, dim=1)\n",
    "#         print(\"h_graph_2\", h_graph_2.shape)\n",
    "\n",
    "        # 6. Last Layer; Combine Graph Embeddings\n",
    "#         print(h_graph_1.shape, h_graph_2.shape)\n",
    "        h_graph = torch.cat([h_graph_1, h_graph_2], dim=1)\n",
    "#         h_graph = h_graph_1\n",
    "\n",
    "        output = self.graph_pred_linear(h_graph)\n",
    "#         print(\"output\", output.shape)\n",
    "\n",
    "        if self.training:\n",
    "            return output\n",
    "        else:\n",
    "            return torch.clamp(output, min=0, max=50)\n",
    "\n",
    "    def gcn_forward_tensorized(self, h, adj, gc_layers, cat=False):\n",
    "        block_readout = []\n",
    "        for gc_layer in gc_layers:\n",
    "            h = gc_layer(h, adj)\n",
    "            block_readout.append(h)\n",
    "        if cat:\n",
    "            block = torch.cat(block_readout, dim=2)  # N x F, F = F1 + F2 + ...\n",
    "        else:\n",
    "            block = h\n",
    "        return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "65311b09-fa70-4c8f-a841-c9e98b2a2d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiffPoolGNN(\n",
       "  (gnn_node): GNN_node_Virtualnode(\n",
       "    (atom_encoder): AtomEncoder(\n",
       "      (atom_embedding_list): ModuleList(\n",
       "        (0): Embedding(119, 600)\n",
       "        (1): Embedding(4, 600)\n",
       "        (2): Embedding(12, 600)\n",
       "        (3): Embedding(12, 600)\n",
       "        (4): Embedding(10, 600)\n",
       "        (5): Embedding(6, 600)\n",
       "        (6): Embedding(6, 600)\n",
       "        (7): Embedding(2, 600)\n",
       "        (8): Embedding(2, 600)\n",
       "      )\n",
       "    )\n",
       "    (virtualnode_embedding): Embedding(1, 600)\n",
       "    (convs): ModuleList(\n",
       "      (0): GINConv(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=600, out_features=600, bias=True)\n",
       "          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=600, out_features=600, bias=True)\n",
       "        )\n",
       "        (bond_encoder): BondEncoder(\n",
       "          (bond_embedding_list): ModuleList(\n",
       "            (0): Embedding(5, 600)\n",
       "            (1): Embedding(6, 600)\n",
       "            (2): Embedding(2, 600)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): GINConv(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=600, out_features=600, bias=True)\n",
       "          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=600, out_features=600, bias=True)\n",
       "        )\n",
       "        (bond_encoder): BondEncoder(\n",
       "          (bond_embedding_list): ModuleList(\n",
       "            (0): Embedding(5, 600)\n",
       "            (1): Embedding(6, 600)\n",
       "            (2): Embedding(2, 600)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): GINConv(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=600, out_features=600, bias=True)\n",
       "          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=600, out_features=600, bias=True)\n",
       "        )\n",
       "        (bond_encoder): BondEncoder(\n",
       "          (bond_embedding_list): ModuleList(\n",
       "            (0): Embedding(5, 600)\n",
       "            (1): Embedding(6, 600)\n",
       "            (2): Embedding(2, 600)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): GINConv(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=600, out_features=600, bias=True)\n",
       "          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=600, out_features=600, bias=True)\n",
       "        )\n",
       "        (bond_encoder): BondEncoder(\n",
       "          (bond_embedding_list): ModuleList(\n",
       "            (0): Embedding(5, 600)\n",
       "            (1): Embedding(6, 600)\n",
       "            (2): Embedding(2, 600)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): GINConv(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=600, out_features=600, bias=True)\n",
       "          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=600, out_features=600, bias=True)\n",
       "        )\n",
       "        (bond_encoder): BondEncoder(\n",
       "          (bond_embedding_list): ModuleList(\n",
       "            (0): Embedding(5, 600)\n",
       "            (1): Embedding(6, 600)\n",
       "            (2): Embedding(2, 600)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (batch_norms): ModuleList(\n",
       "      (0): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (mlp_virtualnode_list): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=600, out_features=600, bias=True)\n",
       "        (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Linear(in_features=600, out_features=600, bias=True)\n",
       "        (4): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=600, out_features=600, bias=True)\n",
       "        (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Linear(in_features=600, out_features=600, bias=True)\n",
       "        (4): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Linear(in_features=600, out_features=600, bias=True)\n",
       "        (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Linear(in_features=600, out_features=600, bias=True)\n",
       "        (4): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Linear(in_features=600, out_features=600, bias=True)\n",
       "        (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Linear(in_features=600, out_features=600, bias=True)\n",
       "        (4): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (pool): SumPooling()\n",
       "  )\n",
       "  (pool): SumPooling()\n",
       "  (graph_pred_linear): Linear(in_features=1200, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DiffPoolGNN(gnn_type='gin', virtual_node=True, **shared_params).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "91df3162-8cd2-48db-a056-18e23dc7c105",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.epochs = 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ca08093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [01:42<00:00,  1.92it/s]\n",
      "Iteration:   0%|                                                                               | 0/196 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0357264785134062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [01:44<00:00,  1.87it/s]\n",
      "Iteration:   0%|                                                                               | 0/196 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5524460212612639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [01:43<00:00,  1.90it/s]\n",
      "Iteration:   0%|                                                                               | 0/196 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4973909178254556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [01:45<00:00,  1.85it/s]\n",
      "Iteration:   0%|                                                                               | 0/196 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4782667038392048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [01:45<00:00,  1.85it/s]\n",
      "Iteration:   0%|                                                                               | 0/196 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4509083230276497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [01:46<00:00,  1.84it/s]\n",
      "Iteration:   0%|                                                                               | 0/196 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45872660376587693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [01:45<00:00,  1.85it/s]\n",
      "Iteration:   0%|                                                                               | 0/196 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4215996349040343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [01:46<00:00,  1.84it/s]\n",
      "Iteration:   0%|                                                                               | 0/196 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4025262977395739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [01:45<00:00,  1.87it/s]\n",
      "Iteration:   0%|                                                                               | 0/196 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5612125732764905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [01:44<00:00,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4172967962768613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train_mae = train(model, device, train_loader, optimizer)\n",
    "\n",
    "loader = train_loader\n",
    "\n",
    "# train\n",
    "model.train()\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    loss_accum = 0\n",
    "    for step, (bg, labels) in enumerate(tqdm(loader, desc=\"Iteration\")):\n",
    "        bg = bg.to(device)\n",
    "        x = bg.ndata.pop('feat').to(device)\n",
    "        edge_attr = bg.edata.pop('feat').to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        pred = model(bg, x, edge_attr).view(-1,)\n",
    "        optimizer.zero_grad()\n",
    "        loss = reg_criterion(pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_accum += loss.detach().cpu().item()\n",
    "\n",
    "    print(loss_accum / (step + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291ac9a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c5a09e3-e312-4b61-8b2b-6f5447dc1e24",
   "metadata": {},
   "source": [
    "## interpret localpooling node mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e9e1aa-f267-48b6-b4cb-7dcee71996f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-rdkit-env",
   "language": "python",
   "name": "my-rdkit-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
